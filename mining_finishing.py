# -*- coding: utf-8 -*-
"""Mining_Finishing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ardilalukita14/Magang-Kominfo/blob/main/Mining_Finishing.ipynb

## **Install Package yang Dibutuhkan**
"""

!pip install newsapi-python

!pip install Sastrawi

!pip install newspaper3k

!pip install wordcloud

"""## **Akses File Stop Words**"""

f = open("stop words.txt", "r")
stopword_list = []
for line in f:
    stripped_line = line.strip()
    line_list = stripped_line.split()
    stopword_list.append(line_list[0])
f.close()

len(stopword_list)

"""## **Import Library yang Dibutuhkan**"""

import pandas as pd
import json
import datetime
import pprint
import requests
from newsapi.newsapi_client import NewsApiClient
from textblob import TextBlob
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

"""## **Input Keyword**"""

# API 
secret = "12fd31f62amshe0539f78a152f87p11c739jsn89c9f491b99e"

# Definisikan endpoint yang akan digunakan untuk mengakses google news
url = 'https://google-news.p.rapidapi.com/v1/search'

# Definisikan atribut di dalam parameters
parameters = {
	'q': input('Masukkan keyword yang diinginkan : '), 
  'country': 'ID', 
  'lang': 'id',
	#'pageSize': 100, # maximum is 100
	#'apiKey': secret # your own API key
}

# Definiskan headers yang akan di proses dalam API
headers= {
    'X-RapidAPI-Key': secret,
    'X-RapidAPI-Host': 'google-news.p.rapidapi.com'
  }
# Make the request
response = requests.get(url,
						params = parameters, headers=headers)

# Convert the response to
# JSON format and pretty print it
# response_json = response.json()
# pprint.pprint(response_json)
# news_data = save_dict('q')
# save_to_csv(news_data)

"""## **Proses Stemmer dan Word Count**"""

import matplotlib.pyplot as plt
response_json = response.json()

text_combined = '' 
factory = StemmerFactory()
stemmer = factory.create_stemmer()

for i in response_json['articles']:
	
	if stemmer.stem(i['title']) != None:
		text_combined += stemmer.stem(i['title']) + ' '
		
wordcount={}
for word in text_combined.split():
	if word not in wordcount:
		wordcount[word] = 1
	else:
		wordcount[word] += 1

for k,v, in sorted(wordcount.items(),
				key=lambda words: words[1],
				reverse = True):
	print(k,v)

"""## **Stop Words**"""

import nltk
import ssl
nltk.download('punkt')
from textblob import TextBlob
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

r = text_combined.replace('\s+',
						' ').replace(',',
									' ').replace('.',
													' ')
words = r.split()
rst = [word for word in words if
	( word.lower() not in stopword_list
		and len(word) > 3) ]

rst = ' '.join(rst)

wordcount={}

for word in rst.split():
	
	if word not in wordcount:
		wordcount[word] = 1
	else:
		wordcount[word] += 1

for k,v, in sorted(wordcount.items(),
				key=lambda words: words[1],
				reverse = True):
	print(k,v)

"""## **Olah File CSV**"""

output = response.json()

articles = output['articles']

df = pd.DataFrame(articles)
df

"""## **Export Files To CSV**"""

df.to_csv(input('Masukkan nama file CSV : '), index=False)

"""## **CSV StopWord**"""

df = pd.DataFrame({'StopWords': [rst]})

df.to_csv(input('Masukkan nama file CSV : '), index=False)

"""## **JOIN Dataset**"""

d1 = pd.read_csv(input('masukkan nama file 1 : '))
d2 = pd.read_csv(input('masukkan nama file 2 : '))
d3 = pd.read_csv(input('masukkan nama file 3 : '))
#d4 = pd.read_csv(input('masukkan nama file 4 : '))
#d5 = pd.read_csv(input('masukkan nama file 5 : '))
frames = [d1, d2, d3] 
result = pd.concat(frames).drop_duplicates().reset_index(drop=True)

result.to_csv(input('Masukkan nama file hasil joining : '), index=False)

df2 = input("Upload file CSV: ")
df3 = pd.read_csv(df2)
df3

"""## **Output (Word Cloud)**"""

# Python program to generate WordCloud
 
# importing all necessary modules
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd
 
# Reads 'Youtube04-Eminem.csv' file
df = pd.read_csv((input('Masukkan file: ')), encoding ="latin-1")
 
comment_words = ''
stopwords = set(STOPWORDS)
 
# iterate through the csv file
for val in df.StopWords:
     
    # typecaste each val to string
    val = str(val)
 
    # split the value
    tokens = val.split()
     
    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
     
    comment_words += " ".join(tokens)+" "
 
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()